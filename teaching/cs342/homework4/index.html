<!DOCTYPE html>
<html lang="en">

  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="Content-Language" content="en">

    <meta name="author" content="Greg Anderson">
    <meta name="description" content="In this homework, we will implement an object detector for the SuperTuxKart simulator. The goal is to look at an image and identify the locations of karts, bombs/projectiles, and pickup items. The dataset is the same as homework 3 and the starting code will look similar. You will need similar data augmentation to what you used in homework 3.
This assignment, as with all of the homework assignments, should be completed individually without sharing solutions, models, or ideas with other students.">
    <meta name="keywords" content="">

    

    <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Homework 4"/>
<meta name="twitter:description" content="In this homework, we will implement an object detector for the SuperTuxKart simulator. The goal is to look at an image and identify the locations of karts, bombs/projectiles, and pickup items. The dataset is the same as homework 3 and the starting code will look similar. You will need similar data augmentation to what you used in homework 3.
This assignment, as with all of the homework assignments, should be completed individually without sharing solutions, models, or ideas with other students."/>

    <meta property="og:title" content="Homework 4" />
<meta property="og:description" content="In this homework, we will implement an object detector for the SuperTuxKart simulator. The goal is to look at an image and identify the locations of karts, bombs/projectiles, and pickup items. The dataset is the same as homework 3 and the starting code will look similar. You will need similar data augmentation to what you used in homework 3.
This assignment, as with all of the homework assignments, should be completed individually without sharing solutions, models, or ideas with other students." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://gavlegoat.github.io/teaching/cs342/homework4/" /><meta property="article:section" content="teaching" />
<meta property="article:published_time" content="2022-03-22T00:00:00+00:00" />
<meta property="article:modified_time" content="2022-03-22T00:00:00+00:00" />


    
      <base href="https://gavlegoat.github.io/teaching/cs342/homework4/">
    
    <title>
  Homework 4 · Greg Anderson
</title>

    
      <link rel="canonical" href="https://gavlegoat.github.io/teaching/cs342/homework4/">
    

    <link href="https://fonts.googleapis.com/css?family=Lato:400,700%7CMerriweather:300,700%7CSource+Code+Pro:400,700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.13.0/css/all.css" integrity="sha384-Bfad6CLCknfcloXFOyFnlgtENryhrpZCe29RTifKEixXQZ38WheV+i/6YWSzkz3V" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.1/normalize.min.css" integrity="sha256-l85OmPOjvil/SOvVt3HnSSjzF1TUMyT9eV0c2BzEGzU=" crossorigin="anonymous" />

    
      
      
      <link rel="stylesheet" href="https://gavlegoat.github.io/css/coder.min.72f30b88c0031425556b74d1c5d02625ace5149101f115edb6639ad27f1eaec8.css" integrity="sha256-cvMLiMADFCVVa3TRxdAmJazlFJEB8RXttmOa0n8ersg=" crossorigin="anonymous" media="screen" />
    

    

    
      
        
        
        <link rel="stylesheet" href="https://gavlegoat.github.io/css/coder-dark.min.717236c74e0a5208ef73964a9f44c6b443b689a95b270d8b2a40d0c012460dac.css" integrity="sha256-cXI2x04KUgjvc5ZKn0TGtEO2ialbJw2LKkDQwBJGDaw=" crossorigin="anonymous" media="screen" />
      
    

    

    

    <link rel="icon" type="image/png" href="https://gavlegoat.github.io/images/favicon-32x32.png" sizes="32x32">
    <link rel="icon" type="image/png" href="https://gavlegoat.github.io/images/favicon-16x16.png" sizes="16x16">

    <meta name="generator" content="Hugo 0.119.0">
  </head>
  
  
  
    
  
  <body class="colorscheme-auto"
        onload=""
  >
    <main class="wrapper">
      <nav class="navigation">
  <section class="container">
    <a class="navigation-title" href="https://gavlegoat.github.io/">
      Greg Anderson
    </a>
    
      <span id="dark-mode-toggle" class="float-right">
        <i class="fas fa-adjust fa-fw"></i>
      </span>
      <input type="checkbox" id="menu-toggle" />
      <label class="menu-button float-right" for="menu-toggle">
        <i class="fas fa-bars fa-fw"></i>
      </label>
      <ul class="navigation-list">
        
          
            <li class="navigation-item">
              <a class="navigation-link" href="https://gavlegoat.github.io/about/">About</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link" href="https://gavlegoat.github.io/research/">Research</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link" href="https://gavlegoat.github.io/publications/">Publications</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link" href="https://gavlegoat.github.io/teaching/">Teaching</a>
            </li>
          
        
        
        <li class="navigation-item separator">
          <span>|</span>
        </li>
      </ul>
    
  </section>
</nav>


      <div class="content">
        
  <section class="container page">
  <article>
    <header>
      <h1>Homework 4</h1>
    </header>

    <p>In this homework, we will implement an object detector for the SuperTuxKart
simulator. The goal is to look at an image and identify the locations of karts,
bombs/projectiles, and pickup items. The dataset is the same as homework 3 and
the starting code will look similar. You will need similar data augmentation to
what you used in homework 3.</p>
<p>This assignment, as with all of the homework assignments, should be completed
individually without sharing solutions, models, or ideas with other students.
See details at the bottom of this page.</p>
<h1 id="starter-code">Starter Code</h1>
<p>We provide some starter code for this homework
<a href="https://gavlegoat.github.io/cs342-homeworks/hw4.zip">here</a>. This zip file contains the following:</p>
<ul>
<li><code>bundle.py</code>, a script you can use to zip up your homework for submission. In
order to submit your homework, run <code>python bundle.py homework &lt;uteid&gt;</code> which
will create a file called <code>&lt;uteid&gt;.zip</code>. This is the file you should submit.</li>
<li><code>grader</code> contains a local grader which you can use to evaluate your homework
and see what grade you would currently get. Note that the local grader and
our grader use different test sets, so this grader is not guaranteed to be
100% accurate.</li>
<li><code>homework</code> contains the code you will modify for this assignment.</li>
</ul>
<p>You can run the local grader at any time by using the following command:</p>
<pre><code>python -m grader homework -v
</code></pre>
<p>We will be losing the same dense dataset that we used in homework 3. That data
should be symlinked to <code>homework4/dense_data/</code>.</p>
<h1 id="point-based-object-detection">Point-based Object Detection</h1>
<p><img src="https://gavlegoat.github.io/cs342-res/box.png" alt="boxes"></p>
<p>Your object detector will be based on your segmentation network from homework 3
(or the master solution). Rather than directly predicting bounding boxes, you
will predict a heatmap of object centers, as in the following image:</p>
<p><img src="https://gavlegoat.github.io/cs342-res/heat.png" alt="heatmap"></p>
<p>Each local maximum (&ldquo;peak&rdquo;) in this heatmap corresponds to the center of an
object in the image.</p>
<h1 id="peak-extraction-20pts">Peak Extraction (20pts)</h1>
<p>In the first part of your homework, you will implement an algorithm to extract
peaks from a heatmap. A &ldquo;peak&rdquo; is any point which is a local maximum in some
(rectangular) neighborhood which has a value above a certain threshold. You
should implement <code>extract_peak</code> in <code>models.py</code> in order to extract these peaks.
The inputs to <code>extract_peak</code> are</p>
<ul>
<li>a heatmap with scores for each pixel,</li>
<li>a window size,</li>
<li>a minimum score for something to be considered a peak, and</li>
<li>The maximum number of peaks to return.</li>
</ul>
<p>If the number of peaks is larger than <code>max_det</code>, then you should return the
<code>max_det</code> peaks with the highest score, where the score is the value in the
heatmap. The return value should be a list of tuples <code>[(s1, x1, y1), ..., (sn, xn, yn)]</code> where each <code>(si, xi, yi)</code> is the score, x-coordinate, and
y-coordinate of a single peak.</p>
<p>Some hints:</p>
<ul>
<li>There may be many points with high scores near the center of an object. Make
sure to only return points which are local maxima.</li>
<li>You can use max pooling to find local maxima. Max pooling expects a 4D tensor
as input &ndash; <code>heatmap[None, None]</code> produces such a tensor.</li>
<li>Use PyTorch functions to implement the peak extractor on the GPU. CPU
implementations may be too slow.</li>
</ul>
<p>Some relevant documentation:</p>
<ul>
<li><a href="https://pytorch.org/docs/stable/generated/torch.nn.functional.max_pool2d.html#torch.nn.functional.max_pool2d">torch.nn.functional.max_pool2d</a></li>
<li><a href="https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk">torch.topk</a></li>
</ul>
<h1 id="object-detection-80pts">Object Detection (80pts)</h1>
<p>Now we&rsquo;ll use the peak extractor to implement object detection. This means
you&rsquo;ll need to design and train a model which produces a heatmap that your peak
extractor can work with.</p>
<h2 id="heatmap-prediction">Heatmap Prediction</h2>
<p>You should design a FCN model which takes an image as input and predicts a
separate heatmap for each class. (That is, your output should have shape <code>[B, C, H, W]</code> where <code>B</code> is the batch size <code>C</code> is the number of classes, <code>H</code> is the
height of the input and <code>W</code> is the width of the input.) Each heatmap should
consist of values in the interval [0, 1] where most values are close to zero,
and high values occur near object centers.</p>
<p>Note that <code>dense_transforms.py</code> includes a transform <code>ToHeatmap</code> which you will
need to use in order to get heatmap labels from the dataset. This
transformation does not change the input image, but just puts the label in a
format you can use. There are several other useful transforms, which are
illustrated by <code>utils.py</code> try running <code>python -m homework.utils</code>.</p>
<p>The labels produced by <code>ToHeatmap</code> include both the heatmap labels and the size
information you might need for the extra credit. That is, iterating over this
dataset yields triples <code>(input, heatmap_label, size_label)</code>.</p>
<p>Some hints:</p>
<ul>
<li>Use sigmoid to produce heatmaps in [0, 1] and use <code>BCEWithLogitsLoss</code> as your
loss function. Note that <code>BCEWithLogitsLoss</code> already includes a sigmoid so
you do not need to apply the sigmoid before the loss function. You will want
to use <code>reduction=&quot;None&quot;</code> with the loss.</li>
<li>Start by implementing <code>__init__</code> and <code>forward</code> as we have been doing with
other models. These are all you need for training. We will worry about
<code>detect</code> later.</li>
<li>Once again, the classes are very unbalanced in this dataset. Try using the
focal loss to improve training.</li>
</ul>
<p>Documentation:</p>
<ul>
<li><a href="https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html#torch.nn.BCEWithLogitsLoss">torch.nn.BCEWithLogitsLoss</a></li>
</ul>
<h2 id="object-detection">Object Detection</h2>
<p>Once you have a heatmap, you are ready to extract object locations from it. To
do this, you should implement <code>detect</code>, which returns a tuple of three lists.
The three lists represent the three classes you are detecting (karts,
projectiles, and pickups) and each element of each list should be a tuple of 5
numbers:</p>
<ul>
<li>The confidence of the detection (a float in the range [0, 1]),</li>
<li>The x and y locations of the center of the object (ints), and</li>
<li>The width and height of the object, each divided by 2 (floats). <em>This part is
for extra credit</em>. If you do not want to return the size of the bounding box,
return 0 for the width and height instead.</li>
</ul>
<p>You should return no more than 30 object locations for each class. If you
detect more than 30 peaks, only return the 30 with the highest scores.</p>
<p>Use <code>extract_peak</code> to get the confidence scores and centers.</p>
<h2 id="evaluation">Evaluation</h2>
<p>Your detector will be evaluated on a metric called average precision (AP). See
<a href="https://towardsdatascience.com/breaking-down-mean-average-precision-map-ae462f623a52">here</a>
or <a href="https://jonathan-hui.medium.com/map-mean-average-precision-for-object-detection-45c121a31173">here</a>
for descriptions of this metric. Briefly, average precision sorts each
detection by its confidence, then measures the precision (percentage of
detections which are correct) and recall (percentage of objects which were
detected) for each class. Average precision is the mean value of the precision
at a recall of 0, 0.1, 0.2, &hellip;, 1. For this homework, we have two separate
criteria for determining whether a prediction is correct:</p>
<ul>
<li>Does the center value you returned lie inside the bounding box of the object?</li>
<li>Is the distance between the predicted center and the ground truch center less
than 5 pixels?</li>
</ul>
<p>Notice that the dataset contains a number of small objects (size smaller than
20 pixels). We ignore these objects and do not penalize predictions that are
close to small objects. These objects are already filtered out of the dataset
inside the provided <code>DetectionSuperTuxDataset</code>.</p>
<p>The specific breakdown of points for this section is:</p>
<ul>
<li>5pts for getting the right output format from your detector,</li>
<li>10pts for the performance of your detector on the inside-the-bounding-box
metric on each class, for a total of 30pts (scored linearly for AP in the ranges
[0.5, 0.75], [0.2, 0.45], and [0.6, 0.85] for the three classes
respectively), and</li>
<li>15pts for the performance of your detector on the center-distance metric
(45pts total, with the same grading as the previous line).</li>
</ul>
<p><strong>WARNING</strong>: The validation set is easier than the test set for this problem.
The master solution achieves AP&rsquo;s of 0.82, 0.61, and 0.89 on the validation set
but only 0.74, 0.53, and 0.89 on the test set. I recommend trying to get your
AP to exceed the maximum values by 0.1.</p>
<h2 id="extra-credit-9pts">Extra Credit 9pts</h2>
<p>You can earn up to 9 points of extra credit by predicting the sizes of objects
along with their centers. This turns your detector into a full-fledged object
detector which outputs bounding boxes rather than just center locations. The
easiest way to do this is to add two more channels to the output of your
network which predict the sizes along with the 3-channel heatmap. This section
is evaluated according to AP at an IoU of 0.5.</p>
<p>Some hints:</p>
<ul>
<li>Add a size loss, but only apply it at object centers.</li>
<li>Carefully balance the size and peak losses.</li>
</ul>
<h1 id="grading">Grading</h1>
<p>As always, the validation grader can be run at any time with</p>
<pre><code>python -m grader homework -v
</code></pre>
<p>Note that this grader is using a different set of test data than we use for our
grader, so there may be some difference between the two. The distributions will
be the same so your grade should be similar between the two graders, unless the
model is massively overfit to the validation set.</p>
<h1 id="submission">Submission</h1>
<p>Once you are ready to submit, create a bundle by running</p>
<pre><code>python bundle.py homework &lt;eid&gt;
</code></pre>
<p>then submit the resulting zip file on Canvas. Note that the grader has a
maximum file size limit of 20MB. You shouldn&rsquo;t run into this limit unless your
models are much larger than they need to be. You can check that your homework
was bundled properly by grading it again with</p>
<pre><code>python -m grader &lt;eid&gt;.zip
</code></pre>
<h1 id="online-grader">Online Grader</h1>
<p>The online grading system uses a slightly modified version of Python, so please
make sure your code follows these rules:</p>
<ul>
<li>Do not use <code>exit</code> or <code>sys.exit</code> as this will likely crash the grader.</li>
<li>Do not try to access files except for the ones provided with the homework zip
file. Writing files is disabled.</li>
<li>Network access is disabled in the grader. Make sure you are reading your
dataset from the <code>data</code> folder rather than from a network connection
somewhere.</li>
<li>Do not fork or spawn new processes.</li>
<li><code>print</code> and <code>sys.stdout.write</code> are ignored by the grader.</li>
</ul>
<h1 id="honor-code">Honor Code</h1>
<p>You should do this homework individually. You are allowed to discuss high-level
ideas and general structure with each other, but not specific details about
code, architecture, or hyperparameters. You may consult online sources, but
don&rsquo;t copy code directly from any posts you find. Cite any ideas you take from
online sources in your code (include the full URL where you found the idea).
You may refer to your own solutions or the master solutions to prior homework
assignments from this class as well as any iPython notebooks from this class.
<strong>Do not put your solution in a public place (e.g., a public GitHub repo)</strong>.</p>
<h1 id="acknowledgements">Acknowledgements</h1>
<p>This assignment is very lightly modified from one created by Philipp
Krähenbühl.</p>

  </article>
</section>

  

      </div>

      
    </main>

    
      
      <script src="https://gavlegoat.github.io/js/dark-mode.min.c2d8a1f8f2660e4a46d776277c72695a1e0ca65939d79f754441d47551604af5.js"></script>
    

    

    

    

    

  </body>

</html>
